# -*- coding: utf-8 -*-
"""chatbot_backend.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bhEnmZFSyhcjJdn1VhPvUdAwtTOgHGc6
"""

# chatbot_backend.py

from transformers import (
    CLIPProcessor,
    CLIPModel,
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline
)
from PIL import Image
import torch
import dotenv
import requests


# ----- Configuration -----
PINECONE_API_KEY = dotenv.get_key(".env", "PINECONE_API_KEY")
PERPLEXITY_API_KEY = dotenv.get_key(".env", "PERPLEXITY_API_KEY")
INDEX_NAME = "multimodal"
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Loaded Pinecone key:", bool(PINECONE_API_KEY))


# ----- Pinecone Initialization -----
from pinecone import Pinecone

print(PINECONE_API_KEY)
pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(INDEX_NAME)


# ----- Load CLIP -----
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# ----- Add Perplexity API function -----
def generate_with_perplexity(prompt):
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "llama-3.1-sonar-large-128k-online",
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful product assistant."
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        "temperature": 0.7,
        "max_tokens": 256
    }
    
    try:
        response = requests.post(
            "https://api.perplexity.ai/chat/completions",
            json=payload,
            headers=headers,
            timeout=10
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except requests.exceptions.HTTPError as e:
        print(f"Perplexity API HTTP Error: {e.response.text}")
        return "Sorry, I couldn't get a response from the AI assistant."
    except Exception as e:
        print(f"Perplexity API error: {str(e)}")
        return "Sorry, there was an error processing your request."

# ----- Embedding Functions -----
def embed_text(text):
    inputs = clip_processor(text=[text], return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        return clip_model.get_text_features(**inputs)[0].cpu().numpy().tolist()

def embed_image(file):
    image = Image.open(file).convert("RGB")
    inputs = clip_processor(images=image, return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        return clip_model.get_image_features(**inputs)[0].cpu().numpy().tolist()

# ----- Main Retrieval + Generation Function -----
def query_vector_db(text=None, image=None):
    if text:
        query_vec = embed_text(text)
        query_type = "text"
        query_text = text
    elif image:
        query_vec = embed_image(image)
        query_type = "image"
        query_text = "What is this product?"
    else:
        return {
            "answer": "Please provide a text or image input.",
            "image_url": None,
            "retrieved_items": []
        }

    # Query Pinecone
    results = index.query(
        vector=query_vec,
        top_k=5,
        include_metadata=True,
        filter={"type": {"$eq": query_type}}
    )

    # Build prompt and product cards
    retrieved_info = ""
    retrieved_items = []

    for match in results.get("matches", []):
        md = match.get("metadata", {})
        name = md.get("name", "Unknown Product")
        category = md.get("category", "Unknown")
        price = md.get("price", "N/A")

        retrieved_info += f"- Name: {name}, Category: {category}, Price: {price}\n"
        retrieved_items.append({
            "title": name,
            "description": f"{category} â€“ ${price}",
            "image": None  # Optionally add URLs if hosted
        })

    # Compose prompt
    prompt = f"""You are a helpful product assistant.

Product Info:
{retrieved_info}

Question:
{query_text}

Answer:"""

    # Generate answer with Perplexity
    answer = generate_with_perplexity(prompt)

    return {
        "answer": answer,
        "image_url": None,
        "retrieved_items": retrieved_items
    }
