# -*- coding: utf-8 -*-
"""chatbot_backend.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bhEnmZFSyhcjJdn1VhPvUdAwtTOgHGc6
"""

# chatbot_backend.py

from transformers import (
    CLIPProcessor,
    CLIPModel,
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline
)
from PIL import Image
import torch
import pinecone
import os

# ----- Configuration -----
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
INDEX_NAME = "multimodal"
LLM_MODEL = "microsoft/phi-2"
device = "cuda" if torch.cuda.is_available() else "cpu"

# ----- Pinecone Initialization -----
pinecone.init(api_key=PINECONE_API_KEY, environment="us-east-1")
index = pinecone.Index(INDEX_NAME)

# ----- Load CLIP -----
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# ----- Load Phi-2 -----
tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)
phi_model = AutoModelForCausalLM.from_pretrained(LLM_MODEL, torch_dtype=torch.float32).to(device)
phi_pipe = pipeline(
    "text-generation",
    model=phi_model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    do_sample=True,
    temperature=0.7,
    device=0 if torch.cuda.is_available() else -1
)

# ----- Embedding Functions -----
def embed_text(text):
    inputs = clip_processor(text=[text], return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        return clip_model.get_text_features(**inputs)[0].cpu().numpy().tolist()

def embed_image(file):
    image = Image.open(file).convert("RGB")
    inputs = clip_processor(images=image, return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        return clip_model.get_image_features(**inputs)[0].cpu().numpy().tolist()

# ----- Main Retrieval + Generation Function -----
def query_vector_db(text=None, image=None):
    if text:
        query_vec = embed_text(text)
        query_type = "text"
        query_text = text
    elif image:
        query_vec = embed_image(image)
        query_type = "image"
        query_text = "What is this product?"
    else:
        return {
            "answer": "Please provide a text or image input.",
            "image_url": None,
            "retrieved_items": []
        }

    # Query Pinecone
    results = index.query(
        vector=query_vec,
        top_k=5,
        include_metadata=True,
        filter={"type": {"$eq": query_type}}
    )

    # Build prompt and product cards
    retrieved_info = ""
    retrieved_items = []

    for match in results.get("matches", []):
        md = match.get("metadata", {})
        name = md.get("name", "Unknown Product")
        category = md.get("category", "Unknown")
        price = md.get("price", "N/A")

        retrieved_info += f"- Name: {name}, Category: {category}, Price: {price}\n"
        retrieved_items.append({
            "title": name,
            "description": f"{category} â€“ ${price}",
            "image": None  # Optionally add URLs if hosted
        })

    # Compose prompt
    prompt = f"""You are a helpful product assistant.

Product Info:
{retrieved_info}

Question:
{query_text}

Answer:"""

    # Generate answer with Phi-2
    generated = phi_pipe(prompt)[0]["generated_text"]
    answer = generated.split("Answer:")[-1].strip()

    return {
        "answer": answer,
        "image_url": None,
        "retrieved_items": retrieved_items
    }